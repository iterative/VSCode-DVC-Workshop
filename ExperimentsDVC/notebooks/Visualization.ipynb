{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7621cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a466b",
   "metadata": {},
   "source": [
    "### Why not merging datasets provided for ML models?\n",
    "\n",
    "In general, when facing the challenge of been given different datasets it is because the **patterns** or **statistical distributions** of training and testing can show differences that have impact on testing different model aspects, such as anomaly detection, overfitting and reactions to data drift. It might also ensure that the **number of samples** for each entity (in this case, Satellites) is tested.\n",
    "\n",
    "The **train_test_split** sklearn function doesn´t contemplate this. If this critera is deprecable in your opinion, let´s merge it.Let me know your thoughts and thanks!   \n",
    "\n",
    "### Goal\n",
    "\n",
    "Show the statistical distribution and differences of training, test and validation dataset and its impact in modeling.\n",
    "\n",
    "For that, \n",
    "\n",
    "In the end, it demonstrates **WHY we shouldn´t merge the datasets into one**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a78145",
   "metadata": {},
   "source": [
    "### Distribution of tested entities ( Satellites) \n",
    "Number of satellites observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3316c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"Reads the data\"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781a09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/jan_train.csv')\n",
    "test = read_data('data/jan_test.csv')\n",
    "key = read_data('data/answer_key.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9530b4",
   "metadata": {},
   "source": [
    "### Number of Unique Satellites in merged train and test, and Splitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23a8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(dataset1, dataset2):\n",
    "    \"\"\"Merge Datasets\"\"\"\n",
    "    frames = [dataset1, dataset2]\n",
    "    dataset_merged = pd.concat(frames)\n",
    "    return dataset_merged\n",
    "    \n",
    "dataset_merged = merge_datasets(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ac8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data, list_column_x, list_column_y):\n",
    "    \"\"\"Select features for training and target features\"\"\"\n",
    "    X = data[list_column_x]\n",
    "    Y = data[list_column_y]\n",
    "    return X , Y\n",
    "\n",
    "list_column_x = ['id', 'sat_id', \n",
    "                 'x_sim','y_sim', 'z_sim', \n",
    "                 'Vx_sim', 'Vy_sim', 'Vz_sim']\n",
    "list_column_y = ['x', 'y', 'z', \n",
    "                 'Vx', 'Vy', 'Vz']\n",
    "\n",
    "X, y = feature_selection(dataset_merged, list_column_x, list_column_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34599da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then, we split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "720ad5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_merged, X_test_merged, y_train_merged, y_test_merged = train_test_split(X, y,\n",
    "                                                                                test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5b589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number training samples merged in train is 519929\n",
      "the number testing samples in merged test is 129983\n"
     ]
    }
   ],
   "source": [
    "print(\"the number training samples merged in train is \" + str(X_train_merged['id'].nunique()))\n",
    "print(\"the number testing samples in merged test is \" + str(X_test_merged['id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5a378fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number training samples in train is 503227\n",
      "the number testing samples in test is 146685\n"
     ]
    }
   ],
   "source": [
    "print(\"the number training samples in train is \" + str(train['id'].nunique()))\n",
    "print(\"the number testing samples in test is \" + str(test['id'].nunique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdfecc4e7fe543b38b768f8e9e67711a4a17cbc715357329a806f7ac49707c05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
