## [PIPELINE STAGES](https://dvc.org/doc/user-guide/pipelines/defining-pipelines#defining-pipelines)

# What are stages ?

## Each one of the data workflows that
## we reproduce reliably to ensure consistent results.
## Might include Feature engineering , train , evaluation

# How are stages defined ?

## The stages entries are
##    cmd: python path that wraps executable shell command.
##    params: loads parameters. Related to params.yaml file
##    deps: dependencies. Might contain Stages that might be executed first
##    outs: outputs, they can be data or even the model

# What do you need to set up your experiments ?

## For setting up the experiments, we are going to follow the data science
## cookie template and we are going to use the folders already given. 


stages:
    load:
        cmd: scr/data/load_dataset.py 
        outs: ExperimentsDVC/data/interim

    featureengineering:
        cmd: scr/data/makedataset.py
        params:
            - features
            - labels 
        deps: scr/data/makeda_dataset.py 
        outs: 
    
    train:
        cmd: scr/models/train_model.py
        params:
            - n_estimators
            - max_depth
            - min_samples_split
            - min_impurity_decrease   
        deps:
        outs: ExperimentsDVC/data/processed
        metrics:    

    evaluate:
        cmd: scr/models/predict_model.py
        params:
        deps:
        outs:       
